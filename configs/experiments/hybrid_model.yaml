# Configuration for hybrid tokenization experiment

data:
  raw_dir: data/raw
  processed_dir: data/processed
  train_split: 0.8
  val_split: 0.1

paleocode_dir: null

tokenizer:
  type: hybrid
  max_length: 512

model:
  type: transformer_encoder
  hidden_size: 384
  num_layers: 6
  num_heads: 6
  feedforward_dim: 1536
  dropout: 0.15

training:
  batch_size: 48
  epochs: 75
  learning_rate: 0.00015
  optimizer: adam
  device: cuda
  num_workers: 4
