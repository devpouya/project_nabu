# Configuration for sign-level Transformer model

data:
  raw_dir: data/raw
  processed_dir: data/processed
  train_split: 0.8
  val_split: 0.1

paleocode_dir: null

tokenizer:
  type: sign
  max_length: 256

model:
  type: transformer_encoder
  hidden_size: 512
  num_layers: 8
  num_heads: 8
  feedforward_dim: 2048
  dropout: 0.1

training:
  batch_size: 32
  epochs: 100
  learning_rate: 0.0001
  optimizer: adam
  device: cuda
  num_workers: 4
  use_scheduler: true
  scheduler_type: cosine
