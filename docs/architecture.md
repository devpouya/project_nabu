---
layout: default
title: Architecture
---

# System Architecture

Project Nabu is built with a modular architecture that separates concerns into distinct components. This design enables flexibility in experimentation and easy extension of capabilities.

## High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Project Nabu                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Datasets   â”‚â”€â”€â”€â–¶â”‚  Tokenizers  â”‚â”€â”€â”€â–¶â”‚    Models    â”‚      â”‚
â”‚  â”‚              â”‚    â”‚              â”‚    â”‚              â”‚      â”‚
â”‚  â”‚ â€¢ ORACC      â”‚    â”‚ â€¢ Sign       â”‚    â”‚ â€¢ Encoder    â”‚      â”‚
â”‚  â”‚ â€¢ CuneiML    â”‚    â”‚ â€¢ Stroke     â”‚    â”‚ â€¢ Decoder    â”‚      â”‚
â”‚  â”‚ â€¢ Custom     â”‚    â”‚ â€¢ Hybrid     â”‚    â”‚              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                             â”‚                                   â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                    â”‚    PaleoCode    â”‚                         â”‚
â”‚                    â”‚   Conversion    â”‚                         â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                             â”‚                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚                                       â”‚               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Encoding   â”‚                       â”‚    Line       â”‚       â”‚
â”‚  â”‚  Pipeline   â”‚                       â”‚  Detection    â”‚       â”‚
â”‚  â”‚ (CV-based)  â”‚                       â”‚  (CNN+Hough)  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. PaleoCode System

The PaleoCode system is the foundation that converts between different representations of cuneiform text.

```
Unicode Cuneiform â†â†’ PaleoCode â†â†’ Strokes
      ğ’€€          â†â†’  h-v:h-v  â†â†’ [h,v,h,v]
```

**Key Classes:**
- `PaleoCodeConverter` - Handles all conversions
- Loads mappings from `paleocodes.json`

**Stroke Primitives:**
| Symbol | Angle Range | Description |
|--------|-------------|-------------|
| `h` | 0-30Â° | Horizontal |
| `v` | 60-90Â° | Vertical |
| `u` | 30-60Â° | Upward diagonal |
| `d` | 120-150Â° | Downward diagonal |
| `c` | Corner | Winkelhaken |

### 2. Tokenization Layer

Three tokenization strategies for different research needs:

#### Sign Tokenizer
```python
"ğ’€€ğ’ˆ¾ğ’† " â†’ [token_AN, token_NA, token_KI]
```
- One token per complete sign
- Best for semantic tasks
- Vocabulary: ~2,000 signs

#### Stroke Tokenizer
```python
"ğ’€€ğ’ˆ¾ğ’† " â†’ [h, v, h, v, h, h, v, h, h, h, v, v]
```
- One token per stroke
- Captures compositional structure
- Vocabulary: ~10 primitives

#### Hybrid Tokenizer
```python
"ğ’€€ğ’ˆ¾ğ’† " â†’ {
    signs: [token_AN, token_NA, token_KI],
    strokes: [h, v, h, v, ...],
    alignment: [(0, 0-4), (1, 4-7), (2, 7-12)]
}
```
- Both representations with alignment
- For hierarchical models

### 3. Dataset System

Unified interface for different cuneiform corpora:

```python
class BaseDataset(ABC):
    @abstractmethod
    def __len__(self) -> int: ...

    @abstractmethod
    def __getitem__(self, idx) -> Tuple[str, Dict]: ...

    @abstractmethod
    def load_data(self) -> None: ...
```

**Implementations:**

| Dataset | Source | Features |
|---------|--------|----------|
| `OraccDataset` | ORACC JSON | Language filtering, metadata |
| `CuneiMLDataset` | CuneiML | Images + annotations |
| `CuneiformDataset` | Text files | Simple loading |

### 4. Model Architectures

#### Transformer Encoder (BERT-style)
```
Input â†’ Embedding â†’ Positional Encoding â†’ N Ã— [Self-Attention + FFN] â†’ Output
```

**Configuration:**
```yaml
model:
  type: transformer_encoder
  vocab_size: auto
  embedding_dim: 512
  num_layers: 6
  num_heads: 8
  feedforward_dim: 2048
  dropout: 0.1
  max_len: 5000
```

#### Transformer Decoder (GPT-style)
```
Input â†’ Embedding â†’ Positional Encoding â†’ N Ã— [Masked Self-Attention + FFN] â†’ Output
```

Uses causal masking to prevent attending to future tokens.

### 5. Encoding Generation Pipeline

Automatic generation of PaleoCode from visual analysis:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Glyph     â”‚â”€â”€â”€â–¶â”‚   Stroke    â”‚â”€â”€â”€â–¶â”‚   Spatial   â”‚â”€â”€â”€â–¶â”‚  Encoding   â”‚
â”‚  Renderer   â”‚    â”‚  Detector   â”‚    â”‚  Analyzer   â”‚    â”‚  Generator  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚                  â”‚                  â”‚
       â–¼                  â–¼                  â–¼                  â–¼
   Binary Image    Detected Lines     Groupings         PaleoCode
   (font render)   (Hough transform)  (sweep-line)      (encoding)
```

**Components:**

1. **GlyphRenderer** - Renders Unicode to binary images using NotoSansCuneiform
2. **StrokeDetector** - Uses Hough lines to find strokes, classifies by angle
3. **SpatialAnalyzer** - Sweep-line algorithm for spatial relationships
4. **EncodingGenerator** - Converts to PaleoCode with operators

### 6. Line Detection System

Two-stage approach for tablet images:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Tablet    â”‚â”€â”€â”€â–¶â”‚  CNN Line   â”‚â”€â”€â”€â–¶â”‚   Hough     â”‚
â”‚   Image     â”‚    â”‚ Segmentationâ”‚    â”‚  Transform  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                  â”‚
                          â–¼                  â–¼
                   Binary Mask        Line Hypotheses
                   (pixel-wise)       (polar coords)
```

**LineNet Architecture:**
```
Conv(1â†’64, 11Ã—11, s=4) â†’ ReLU â†’ MaxPool â†’ LRN
â†’ Conv(64â†’256, 5Ã—5) â†’ ReLU â†’ MaxPool â†’ LRN
â†’ Conv(256â†’384, 3Ã—3) â†’ BN â†’ ReLU
â†’ Conv(384â†’384, 3Ã—3) â†’ BN â†’ ReLU
â†’ Conv(384â†’256, 3Ã—3) â†’ BN â†’ ReLU â†’ MaxPool
â†’ FC(9216â†’512) â†’ ReLU â†’ Dropout
â†’ FC(512â†’2) â†’ Softmax
```

## Data Flow

### Training Pipeline

```
1. Load Config (YAML)
        â”‚
2. Initialize Dataset
        â”‚
3. Build Tokenizer Vocabulary
        â”‚
4. Create DataLoaders
        â”‚
5. Initialize Model
        â”‚
6. Training Loop:
   â”‚
   â”œâ”€â”€ Forward Pass
   â”œâ”€â”€ Compute Loss
   â”œâ”€â”€ Backward Pass
   â”œâ”€â”€ Update Weights
   â”œâ”€â”€ Log Metrics (TensorBoard/W&B)
   â””â”€â”€ Save Checkpoints
        â”‚
7. Evaluation
        â”‚
8. Export Model
```

### Inference Pipeline

```
Input Text (Unicode)
        â”‚
        â–¼
   PaleoCode Conversion
        â”‚
        â–¼
   Tokenization (Sign/Stroke/Hybrid)
        â”‚
        â–¼
   Model Forward Pass
        â”‚
        â–¼
   Output (Classification/Generation)
```

## Configuration System

YAML-based configuration for reproducible experiments:

```yaml
# configs/experiments/example.yaml

# Data
data:
  train_path: data/train.txt
  val_path: data/val.txt
  test_path: data/test.txt

# Tokenizer
tokenizer:
  type: sign  # sign | stroke | hybrid
  max_length: 512
  vocab_min_freq: 2

# Model
model:
  type: transformer_encoder  # transformer_encoder | transformer_decoder
  hidden_size: 256
  num_layers: 6
  num_heads: 8
  feedforward_dim: 1024
  dropout: 0.1

# Training
training:
  batch_size: 32
  epochs: 100
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_steps: 1000
  gradient_clip: 1.0
  device: cuda

# Checkpointing
checkpointing:
  save_dir: outputs/checkpoints
  save_every: 10
  keep_last: 3

# Logging
logging:
  log_dir: outputs/logs
  log_every: 100
```

## Extension Points

### Adding a New Tokenizer

```python
from nabu.tokenizers.base import BaseTokenizer

class MyTokenizer(BaseTokenizer):
    def build_vocab(self, texts):
        # Build vocabulary from texts
        pass

    def encode(self, text):
        # Convert text to token IDs
        pass

    def decode(self, ids):
        # Convert token IDs to text
        pass
```

### Adding a New Dataset

```python
from nabu.datasets.base import BaseDataset

class MyDataset(BaseDataset):
    def load_data(self):
        # Load your data source
        pass

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.metadata[idx]
```

### Adding a New Model

```python
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Define layers

    def forward(self, x, attention_mask=None):
        # Forward pass
        return output
```

## Design Principles

1. **Modularity** - Components can be mixed and matched
2. **Configuration-Driven** - Experiments defined in YAML
3. **Extensibility** - Easy to add new tokenizers, datasets, models
4. **Reproducibility** - Checkpointing and logging built-in
5. **Multi-Scale** - Support for sign-level to stroke-level analysis

## File Organization

```
src/nabu/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ tokenizers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py           # Abstract base class
â”‚   â”œâ”€â”€ sign_tokenizer.py
â”‚   â”œâ”€â”€ stroke_tokenizer.py
â”‚   â””â”€â”€ hybrid_tokenizer.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ transformer.py    # Encoder and Decoder
â”‚   â””â”€â”€ embeddings.py     # Token and positional embeddings
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py           # Abstract base class
â”‚   â”œâ”€â”€ oracc_dataset.py
â”‚   â”œâ”€â”€ cuneiml_dataset.py
â”‚   â””â”€â”€ line_dataset.py
â”œâ”€â”€ encoding/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ glyph_renderer.py
â”‚   â”œâ”€â”€ stroke_detector.py
â”‚   â”œâ”€â”€ spatial_analyzer.py
â”‚   â””â”€â”€ encoding_generator.py
â”œâ”€â”€ detection/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ linenet.py
â”‚   â””â”€â”€ line_detection.py
â”œâ”€â”€ paleocode/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ paleocode.py
â”œâ”€â”€ dataloaders/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ builders.py
â”‚   â””â”€â”€ collate.py
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ experiment_tracker.py
```
